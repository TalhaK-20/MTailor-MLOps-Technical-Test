"""
Local ONNX Model Testing Script
Tests the converted ONNX model with sample images before deployment
"""

import os
import sys
import time
import argparse
from pathlib import Path
import numpy as np
from PIL import Image
import logging

# Import our model handler
from model import ONNXModelHandler

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class LocalModelTester:
    def __init__(self, model_path: str):
        """Initialize the local model tester"""
        self.model_path = model_path
        self.model_handler = None
        self.test_results = []

    def setup_model(self):
        """Load and setup the ONNX model"""
        try:
            logger.info(f"Loading ONNX model from: {self.model_path}")
            self.model_handler = ONNXModelHandler(self.model_path)
            logger.info("‚úÖ Model loaded successfully")
            return True
        except Exception as e:
            logger.error(f"‚ùå Failed to load model: {str(e)}")
            return False

    def create_test_images(self, output_dir: str = "test_images"):
        """Create sample test images if they don't exist"""
        os.makedirs(output_dir, exist_ok=True)

        # Create synthetic test images
        test_cases = [
            ("red_square.jpg", (255, 0, 0)),
            ("green_circle.jpg", (0, 255, 0)),
            ("blue_triangle.jpg", (0, 0, 255)),
            ("gradient.jpg", None)  # Special case for gradient
        ]

        for filename, color in test_cases:
            filepath = os.path.join(output_dir, filename)
            if not os.path.exists(filepath):
                if color:
                    # Create solid color image
                    img = Image.new('RGB', (224, 224), color)
                else:
                    # Create gradient image
                    img_array = np.zeros((224, 224, 3), dtype=np.uint8)
                    for i in range(224):
                        for j in range(224):
                            img_array[i, j] = [i % 256, j % 256, (i + j) % 256]
                    img = Image.fromarray(img_array)

                img.save(filepath)
                logger.info(f"Created test image: {filepath}")

        return [os.path.join(output_dir, case[0]) for case in test_cases]

    def test_single_image(self, image_path: str):
        """Test prediction on a single image"""
        try:
            logger.info(f"Testing image: {image_path}")

            # Time the prediction
            start_time = time.time()
            predicted_class, confidence, probabilities = self.model_handler.predict(image_path)
            inference_time = time.time() - start_time

            # Store results
            result = {
                'image_path': image_path,
                'predicted_class': predicted_class,
                'confidence': confidence,
                'inference_time': inference_time,
                'top_5_probs': probabilities[:5] if len(probabilities) > 5 else probabilities,
                'status': 'success'
            }

            logger.info(f"‚úÖ Prediction: {predicted_class} (confidence: {confidence:.4f}, time: {inference_time:.3f}s)")
            return result

        except Exception as e:
            logger.error(f"‚ùå Failed to predict {image_path}: {str(e)}")
            return {
                'image_path': image_path,
                'error': str(e),
                'status': 'error'
            }

    def test_batch_images(self, image_paths: list):
        """Test prediction on multiple images"""
        logger.info(f"Testing {len(image_paths)} images...")

        for image_path in image_paths:
            if os.path.exists(image_path):
                result = self.test_single_image(image_path)
                self.test_results.append(result)
            else:
                logger.warning(f"Image not found: {image_path}")

    def benchmark_performance(self, image_path: str, num_runs: int = 10):
        """Benchmark model performance"""
        logger.info(f"Benchmarking performance with {num_runs} runs...")

        if not os.path.exists(image_path):
            logger.error(f"Benchmark image not found: {image_path}")
            return None

        times = []
        for i in range(num_runs):
            start_time = time.time()
            try:
                _, _, _ = self.model_handler.predict(image_path)
                inference_time = time.time() - start_time
                times.append(inference_time)
            except Exception as e:
                logger.error(f"Benchmark run {i + 1} failed: {str(e)}")
                continue

        if times:
            avg_time = np.mean(times)
            std_time = np.std(times)
            min_time = np.min(times)
            max_time = np.max(times)

            benchmark_results = {
                'num_runs': len(times),
                'avg_time': avg_time,
                'std_time': std_time,
                'min_time': min_time,
                'max_time': max_time,
                'fps': 1.0 / avg_time
            }

            logger.info(f"üìä Benchmark Results:")
            logger.info(f"   Average time: {avg_time:.4f}s ¬± {std_time:.4f}s")
            logger.info(f"   Min/Max time: {min_time:.4f}s / {max_time:.4f}s")
            logger.info(f"   Throughput: {1.0 / avg_time:.2f} FPS")

            return benchmark_results

        return None

    def validate_model_outputs(self):
        """Validate that model outputs are reasonable"""
        logger.info("üîç Validating model outputs...")

        # Check if we have any successful predictions
        successful_tests = [r for r in self.test_results if r.get('status') == 'success']

        if not successful_tests:
            logger.error("‚ùå No successful predictions to validate!")
            return False

        validation_passed = True

        for result in successful_tests:
            confidence = result.get('confidence', 0)
            predicted_class = result.get('predicted_class', '')

            # Validate confidence is between 0 and 1
            if not (0 <= confidence <= 1):
                logger.error(f"‚ùå Invalid confidence value: {confidence}")
                validation_passed = False

            # Validate predicted class is not empty
            if not predicted_class or not isinstance(predicted_class, str):
                logger.error(f"‚ùå Invalid predicted class: {predicted_class}")
                validation_passed = False

            # Validate probabilities sum to ~1 (if available)
            if 'top_5_probs' in result:
                prob_sum = sum(p[1] for p in result['top_5_probs'])
                if not (0.8 <= prob_sum <= 1.2):  # Allow some tolerance
                    logger.warning(f"‚ö†Ô∏è Probabilities don't sum to 1: {prob_sum}")

        if validation_passed:
            logger.info("‚úÖ Model output validation passed!")
        else:
            logger.error("‚ùå Model output validation failed!")

        return validation_passed

    def generate_test_report(self):
        """Generate a comprehensive test report"""
        logger.info("üìã Generating test report...")

        successful_tests = [r for r in self.test_results if r.get('status') == 'success']
        failed_tests = [r for r in self.test_results if r.get('status') == 'error']

        report = {
            'model_path': self.model_path,
            'total_tests': len(self.test_results),
            'successful_tests': len(successful_tests),
            'failed_tests': len(failed_tests),
            'success_rate': len(successful_tests) / len(self.test_results) if self.test_results else 0,
            'average_inference_time': np.mean(
                [r['inference_time'] for r in successful_tests]) if successful_tests else 0,
            'test_results': self.test_results
        }

        # Print summary
        print("\n" + "=" * 60)
        print("üß™ LOCAL MODEL TEST REPORT")
        print("=" * 60)
        print(f"Model Path: {report['model_path']}")
        print(f"Total Tests: {report['total_tests']}")
        print(f"Successful: {report['successful_tests']}")
        print(f"Failed: {report['failed_tests']}")
        print(f"Success Rate: {report['success_rate']:.2%}")
        print(f"Avg Inference Time: {report['average_inference_time']:.4f}s")

        if successful_tests:
            print(f"Estimated Throughput: {1 / report['average_inference_time']:.2f} FPS")

        print("\nüìä Individual Test Results:")
        for i, result in enumerate(self.test_results, 1):
            status_icon = "‚úÖ" if result['status'] == 'success' else "‚ùå"
            image_name = os.path.basename(result['image_path'])

            if result['status'] == 'success':
                print(
                    f"{i:2d}. {status_icon} {image_name:<20} -> {result['predicted_class']:<15} ({result['confidence']:.4f}) [{result['inference_time']:.3f}s]")
            else:
                print(f"{i:2d}. {status_icon} {image_name:<20} -> ERROR: {result['error']}")

        print("=" * 60)

        return report


def main():
    parser = argparse.ArgumentParser(description='Test ONNX model locally before deployment')
    parser.add_argument('--model', type=str, default='model.onnx', help='Path to ONNX model file')
    parser.add_argument('--test_images', type=str, help='Directory containing test images')
    parser.add_argument('--create_samples', action='store_true', help='Create sample test images')
    parser.add_argument('--benchmark', action='store_true', help='Run performance benchmark')
    parser.add_argument('--benchmark_runs', type=int, default=10, help='Number of benchmark runs')

    args = parser.parse_args()

    # Check if model exists
    if not os.path.exists(args.model):
        logger.error(f"‚ùå Model file not found: {args.model}")
        logger.info("üí° Make sure to run convert_to_onnx.py first to create the ONNX model")
        sys.exit(1)

    # Initialize tester
    tester = LocalModelTester(args.model)

    # Setup model
    if not tester.setup_model():
        logger.error("‚ùå Failed to setup model. Exiting.")
        sys.exit(1)

    # Create or find test images
    if args.create_samples or not args.test_images:
        test_images = tester.create_test_images()
    else:
        # Use provided test images directory
        if os.path.isdir(args.test_images):
            test_images = [os.path.join(args.test_images, f) for f in os.listdir(args.test_images)
                           if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
        else:
            logger.error(f"‚ùå Test images directory not found: {args.test_images}")
            sys.exit(1)

    if not test_images:
        logger.error("‚ùå No test images found!")
        sys.exit(1)

    # Run tests
    tester.test_batch_images(test_images)

    # Validate outputs
    tester.validate_model_outputs()

    # Run benchmark if requested
    if args.benchmark and test_images:
        tester.benchmark_performance(test_images[0], args.benchmark_runs)

    # Generate report
    report = tester.generate_test_report()

    # Exit with appropriate code
    if report['success_rate'] < 1.0:
        logger.warning("‚ö†Ô∏è Some tests failed. Please review the results.")
        sys.exit(1)
    else:
        logger.info("üéâ All tests passed! Model is ready for deployment.")
        sys.exit(0)


if __name__ == "__main__":
    main()